{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Single-GPU Training with Amazon SageMaker on Amazon SageMaker Studio Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import essentials packages, start a sagemaker session and specify the bucket name you created in the pre-requsites section of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import numpy as np\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using temporary credentials, copy the output of `aws sts get-session-token --duration-seconds <DURATION>` and assign it to the `credentials` variable below.\n",
    "\n",
    "If you're using long-term credentials, ignore this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"Credentials\": {\n",
    "        \"AccessKeyId\": \"<AWS_ACCESS_KEY_ID>\",\n",
    "        \"SecretAccessKey\": \"<AWS_SECRET_ACCESS_KEY>\",\n",
    "        \"SessionToken\": \"<AWS_SESSION_TOKEN>\",\n",
    "    }\n",
    "}\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = credentials['Credentials']['AccessKeyId']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = credentials['Credentials']['SecretAccessKey']\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = credentials['Credentials']['SessionToken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the sagemaker studio lab ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_role='arn:aws:iam::<ACCOUNT_ID>:role/SageMakerStudioLabRole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm   = sess.client('sagemaker')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker_role\n",
    "\n",
    "bucket_name    = sagemaker_session.default_bucket()\n",
    "jobs_folder    = 'jobs'\n",
    "dataset_folder = 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1000/0*GRfvsrvtfpRm400-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_cifar10_tfrecords.py --data-dir=cifar10-dataset\n",
    "datasets = sagemaker_session.upload_data(path='cifar10-dataset', key_prefix=f'{dataset_folder}/cifar10-tfrecords')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Specify hyperparameters, instance type and number of instances to distribute training to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name   = f'tf-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S-%j\", time.gmtime())}'\n",
    "output_path = f's3://{bucket_name}/{jobs_folder}'\n",
    "\n",
    "metric_definitions = [{'Name': 'Validation Accuracy', 'Regex': 'Validation Accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "hyperparameters = {'epochs'       : 50, \n",
    "                   'learning-rate': 0.01,\n",
    "                   'momentum'     : 0.95,\n",
    "                   'weight-decay' : 2e-4,\n",
    "                   'optimizer'    : 'adam',\n",
    "                   'batch-size'   : 256,\n",
    "                   'model-type'   : 'custom'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f's3://{bucket_name}/tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(entry_point         = 'cifar10-tf2.py', \n",
    "                           source_dir           = 'code',\n",
    "                           output_path          = output_path + '/',\n",
    "                           code_location        = output_path,\n",
    "                           role                 = role,\n",
    "                           instance_count       = 1,\n",
    "                           instance_type        = 'ml.p3.2xlarge',\n",
    "                           framework_version    = '2.4', \n",
    "                           py_version           = 'py37',\n",
    "                           metric_definitions   = metric_definitions,\n",
    "                           hyperparameters      = hyperparameters,\n",
    "                           tensorboard_output_config = tensorboard_output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Specify dataset locations in Amazon S3 and then call the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': datasets,\n",
    "                    'validation': datasets,\n",
    "                    'eval': datasets}, \n",
    "                  job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete S3 artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "s3_bucket.objects.filter(Prefix=f\"{jobs_folder}/\").delete()\n",
    "s3_bucket.objects.filter(Prefix=f\"{dataset_folder}/\").delete()\n",
    "\n",
    "print(f\"\\nDeleted contents of {bucket}/{jobs_folder}\")\n",
    "print(f\"\\nDeleted contents of {bucket}/{dataset_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
